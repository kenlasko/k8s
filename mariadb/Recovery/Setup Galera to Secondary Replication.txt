# Select the MariaDB pod on NUC6 and go to command prompt:
mariadb -u root -p***REMOVED***
flush tables with read lock;
show variables like 'gtid_binlog_pos';  

# Take results from above and set gtid_slave_pos for last step

# From another window on same cluster member:
mariadb-dump -h mariadb.mariadb.svc.cluster.local -u root -p***REMOVED*** -B gitea homeassist phpmyadmin ucdialplans vaultwarden > /bitnami/mariadb/data/mariadb-repl-backup.sql

# Once done, then unlock from first:
unlock tables;

# From NUC3 host
sudo scp ken@nuc6:/kube-storage/mariadb/data/mariadb-repl-backup.sql /kube-storage/mariadb/mariadb-repl-backup.sql

# From NAS01 host
sudo scp ken@nuc6:/kube-storage/mariadb/data/mariadb-repl-backup.sql /share/appdata/docker-vol/mariadb/backup/mariadb-repl-backup.sql

# Then on mariadb-standalone pod command prompt:
mariadb -u root -p***REMOVED***
stop slave;

drop database gitea;
drop database homeassist;
drop database ucdialplans;
drop database vaultwarden;
drop database phpmyadmin;

# Exit to prompt and run
mariadb -u root -p***REMOVED*** < /bitnami/mariadb/mariadb-repl-backup.sql

# Then run 
mariadb -u root -p***REMOVED***
set global gtid_slave_pos = "0-1-3853320,1-1-42";
change master to
    master_host='mariadb.mariadb.svc.cluster.local',
    master_user='replicator',
    master_password='***REMOVED***',
    master_port=3306,
    master_connect_retry=10,
    master_use_gtid=slave_pos;

start slave;

# Check status
show slave status;

# If you get replication errors, try skipping the error and continuing:
STOP SLAVE;
SET GLOBAL SQL_SLAVE_SKIP_COUNTER = 1;
START SLAVE;
SELECT SLEEP(5);
SHOW SLAVE STATUS;